{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INF 721 - Final Project**\n",
        "\n",
        "**Student:** Bruno Alencar Vieira de Rezende\n",
        "\n",
        "**Enrollment Number:** ES102008"
      ],
      "metadata": {
        "id": "oXudq0ATc50p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This work implements the Transformer model introduced in \"Attention Is All You Need\" by Vaswani et al. (2017) using PyTorch, focusing on testing the results achieved on the WMT 2014 English-to-German translation task. The project includes dataset preprocessing, environment setup, and building key Transformer components like multi-head attention and the encoder-decoder structure, while adapting training to resource constraints.\n"
      ],
      "metadata": {
        "id": "MnIv_zT8dSNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Py5tfAw8dxtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0CTVmrdI4i3",
        "outputId": "3ccba1be-040c-4689-837c-b380b21586d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzWh7lV1dIlP",
        "outputId": "96db7733-6db4-4bff-e097-3209742f7900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "IHQ1TRaGJIKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "hlTgsY4JJOhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wmt14\", \"de-en\")"
      ],
      "metadata": {
        "id": "J-WV7FUtJQPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'] = dataset['train'].select(range(500000))"
      ],
      "metadata": {
        "id": "ERvJ3FMzJWkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture"
      ],
      "metadata": {
        "id": "umefzeInJbJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    embed_size,\n",
        "    num_heads\n",
        "  ):\n",
        "    super(SelfAttention, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_size // num_heads\n",
        "\n",
        "    self.values = nn.Linear(embed_size, embed_size)\n",
        "    self.keys = nn.Linear(embed_size, embed_size)\n",
        "    self.queries = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    values,\n",
        "    keys,\n",
        "    queries,\n",
        "    mask\n",
        "  ):\n",
        "    n = queries.shape[0] # number of training examples\n",
        "\n",
        "    values_len, keys_len, queries_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "\n",
        "    values = values.reshape(n, values_len, self.num_heads, self.head_dim)\n",
        "    keys = keys.reshape(n, keys_len, self.num_heads, self.head_dim)\n",
        "    queries = queries.reshape(n, queries_len, self.num_heads, self.head_dim)\n",
        "\n",
        "    temp = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "    if mask is not None:\n",
        "      MINUS_INF = float(\"-1e16\")\n",
        "      temp = temp.masked_fill(mask == 0, MINUS_INF)\n",
        "\n",
        "    attention = torch.softmax(temp / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "    out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
        "\n",
        "    # concatenate\n",
        "    out = out.reshape(n, queries_len, self.num_heads * self.head_dim)\n",
        "\n",
        "    out = self.fc_out(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "qF2ZwT56dPtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    embed_size,\n",
        "    num_heads,\n",
        "    dropout,\n",
        "    forward_expansion\n",
        "  ):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, num_heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "      nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(forward_expansion * embed_size, embed_size),\n",
        "    )\n",
        "\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,\n",
        "    values,\n",
        "    keys,\n",
        "    queries,\n",
        "    mask\n",
        "  ):\n",
        "    attention_out = self.attention(values, keys, queries, mask)\n",
        "\n",
        "    norm1_out = self.dropout(self.norm1(attention_out + queries)) # the queries represent the skip connection\n",
        "\n",
        "    feed_forward_out = self.feed_forward(norm1_out)\n",
        "\n",
        "    out = self.dropout(self.norm2(feed_forward_out + norm1_out)) # the norm1_out represent the skip connection\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "M_W9xrXIj69B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    vocab_size,\n",
        "    embed_size,\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    device,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_length # for positional embedding\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "      [TransformerBlock(embed_size, num_heads, dropout=dropout, forward_expansion=forward_expansion) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    x,\n",
        "    mask\n",
        "  ):\n",
        "    n, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)\n",
        "\n",
        "    out = self.word_embedding(x) + self.position_embedding(positions)\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask) # values, keys and queries are all the same\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "MekRWVhAm_Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      embed_size,\n",
        "      num_heads,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      device\n",
        "    ):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = SelfAttention(embed_size, num_heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(\n",
        "      embed_size, num_heads, dropout, forward_expansion\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    x,\n",
        "    value,\n",
        "    key,\n",
        "    src_mask,\n",
        "    trg_mask\n",
        "  ):\n",
        "    attention_out = self.attention(x, x, x, trg_mask)\n",
        "    norm_out = self.dropout(self.norm(attention_out + x)) # the x represents the skip connection\n",
        "    out = self.transformer_block(value, key, norm_out, src_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "mH0UZ2iYot98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    vocab_size,\n",
        "    embed_size,\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    device,\n",
        "    max_length,\n",
        "  ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.device = device\n",
        "    self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "    self.layers = nn.ModuleList(\n",
        "      [DecoderBlock(embed_size, num_heads, forward_expansion, dropout, device) for _ in range(num_layers)]\n",
        "    )\n",
        "    self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    x,\n",
        "    enc_out,\n",
        "    src_mask,\n",
        "    trg_mask\n",
        "  ):\n",
        "    n, seq_length = x.shape\n",
        "    positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)\n",
        "    x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "    out = self.fc_out(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "dcTw5uy3pv6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    trg_pad_idx,\n",
        "    embed_size=512,\n",
        "    num_layers=6,\n",
        "    forward_expansion=4,\n",
        "    heads=8,\n",
        "    dropout=0,\n",
        "    device=\"cpu\",\n",
        "    max_length=100,\n",
        "  ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(\n",
        "      src_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      device,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      max_length,\n",
        "    )\n",
        "    self.decoder = Decoder(\n",
        "      trg_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      forward_expansion,\n",
        "      dropout,\n",
        "      device,\n",
        "      max_length,\n",
        "    )\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(\n",
        "    self,\n",
        "    src\n",
        "  ):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "  def make_trg_mask(\n",
        "    self,\n",
        "    trg\n",
        "  ):\n",
        "    n, trg_len = trg.shape\n",
        "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(n, 1, trg_len, trg_len)\n",
        "\n",
        "    return trg_mask.to(self.device)\n",
        "\n",
        "  def forward(self,\n",
        "    src,\n",
        "    trg\n",
        "  ):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "Cr5MuxJarGkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "YhcfsddgIyHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(data, language, vocab_size=10000):\n",
        "  token_counter = Counter()\n",
        "  for translation in data[\"translation\"]:\n",
        "    tokens = word_tokenize(translation[language].lower())\n",
        "    token_counter.update(tokens)\n",
        "  most_common = token_counter.most_common(vocab_size - 4)\n",
        "  vocab = {word: idx + 4 for idx, (word, _) in enumerate(most_common)}\n",
        "  vocab[\"<PAD>\"] = 0 # padding\n",
        "  vocab[\"<SOS>\"] = 1 # start of sequence\n",
        "  vocab[\"<EOS>\"] = 2 # end of sequence\n",
        "  vocab[\"<UNK>\"] = 3 # unknown\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "x_JQADASJnvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = build_vocab(dataset[\"train\"], \"en\")\n",
        "trg_vocab = build_vocab(dataset[\"train\"], \"de\")"
      ],
      "metadata": {
        "id": "8iEp0d4RLFwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_en = {v: k for k, v in src_vocab.items()}\n",
        "id_to_de = {v: k for k, v in trg_vocab.items()}"
      ],
      "metadata": {
        "id": "tUQN2bWdTZ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_translation(example, src_vocab, trg_vocab):\n",
        "  src_text = word_tokenize(example[\"translation\"][\"en\"].lower())\n",
        "  trg_text = word_tokenize(example[\"translation\"][\"de\"].lower())\n",
        "\n",
        "  src_ids = [src_vocab.get(token, src_vocab[\"<UNK>\"]) for token in src_text]\n",
        "  trg_ids = [trg_vocab.get(token, trg_vocab[\"<UNK>\"]) for token in trg_text]\n",
        "\n",
        "  return (\n",
        "    [src_vocab[\"<SOS>\"]] + src_ids + [src_vocab[\"<EOS>\"]],\n",
        "    [trg_vocab[\"<SOS>\"]] + trg_ids + [trg_vocab[\"<EOS>\"]],\n",
        "  )\n",
        "\n",
        "train_data_pairs = [\n",
        "    preprocess_translation(example, src_vocab, trg_vocab) for example in dataset[\"train\"]\n",
        "]\n",
        "validation_data_pairs = [\n",
        "    preprocess_translation(example, src_vocab, trg_vocab) for example in dataset[\"validation\"]\n",
        "]\n",
        "test_data_pairs = [\n",
        "    preprocess_translation(example, src_vocab, trg_vocab) for example in dataset[\"test\"]\n",
        "]"
      ],
      "metadata": {
        "id": "HYxM3YRoL529"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_pairs[0])\n",
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJzddK5PMbNR",
        "outputId": "9e8a0257-784c-420a-b0d2-efb4f36561fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([1, 5090, 7, 4, 1621, 2], [1, 3606, 7, 3137, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode',\n",
              "  'en': 'Resumption of the session'}}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequence(seq, max_length, pad_idx):\n",
        "  return seq + [pad_idx] * (max_length - len(seq)) if len(seq) < max_length else seq[:max_length]"
      ],
      "metadata": {
        "id": "Gd0ByrYhMtb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "  def __init__(self, data, src_pad_idx, trg_pad_idx, max_length=50):\n",
        "    self.data = data\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src, trg = self.data[idx]\n",
        "    src = pad_sequence(src, self.max_length, self.src_pad_idx)\n",
        "    trg = pad_sequence(trg, self.max_length, self.trg_pad_idx)\n",
        "    return torch.tensor(src), torch.tensor(trg)"
      ],
      "metadata": {
        "id": "8A13RS1vM1ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_pad_idx = src_vocab[\"<PAD>\"]\n",
        "trg_pad_idx = trg_vocab[\"<PAD>\"]\n",
        "\n",
        "train_dataset = TranslationDataset(train_data_pairs, src_pad_idx, trg_pad_idx)\n",
        "validation_dataset = TranslationDataset(validation_data_pairs, src_pad_idx, trg_pad_idx)\n",
        "test_dataset = TranslationDataset(test_data_pairs, src_pad_idx, trg_pad_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "d6JICcsVNFHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranining The Transformer"
      ],
      "metadata": {
        "id": "EfkKu58fNLOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(loader, model, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for src, trg in loader:\n",
        "      src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "      output = model(src, trg[:, :-1])\n",
        "      output = output.reshape(-1, output.shape[2])\n",
        "      trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "      loss = criterion(output, trg)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "EzA-rHcIV_5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(src_vocab)\n",
        "trg_vocab_size = len(trg_vocab)\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    trg_vocab_size=trg_vocab_size,\n",
        "    src_pad_idx=src_pad_idx,\n",
        "    trg_pad_idx=trg_pad_idx,\n",
        "    device=device,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  loop = tqdm(train_loader, leave=True)\n",
        "  epoch_loss = 0\n",
        "  for batch_idx, (src, trg) in enumerate(loop):\n",
        "    src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "    output = model(src, trg[:, :-1])\n",
        "    output = output.reshape(-1, output.shape[2])\n",
        "    trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "  val_loss = evaluate_model(validation_loader, model, criterion, device)\n",
        "  print(f\"Epoch {epoch+1} completed. Training Loss: {epoch_loss / len(train_loader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "test_loss = evaluate_model(test_loader, model, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqwIwjZVNKbT",
        "outputId": "8aa18c58-167a-498c-cd2a-400a31b9a7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/3]: 100%|██████████| 7813/7813 [40:38<00:00,  3.20it/s, loss=1.97]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Training Loss: 2.9196, Validation Loss: 2.5239\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/3]: 100%|██████████| 7813/7813 [40:33<00:00,  3.21it/s, loss=2.07]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. Training Loss: 1.9441, Validation Loss: 2.3028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/3]: 100%|██████████| 7813/7813 [40:32<00:00,  3.21it/s, loss=1.57]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed. Training Loss: 1.7412, Validation Loss: 2.2023\n",
            "Test Loss: 2.1899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, src_vocab, trg_vocab, src_pad_idx, trg_pad_idx, max_length=50, device=\"cpu\"):\n",
        "  model.eval()\n",
        "\n",
        "  tokens = word_tokenize(sentence.lower())\n",
        "  src_ids = [src_vocab.get(token, src_vocab[\"<UNK>\"]) for token in tokens]\n",
        "  src_ids = [src_vocab[\"<SOS>\"]] + src_ids + [src_vocab[\"<EOS>\"]]\n",
        "\n",
        "  src_ids = src_ids + [src_pad_idx] * (max_length - len(src_ids)) if len(src_ids) < max_length else src_ids[:max_length]\n",
        "  src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  trg_ids = [trg_vocab[\"<SOS>\"]]\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    trg_tensor = torch.tensor(trg_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model(src_tensor, trg_tensor)\n",
        "\n",
        "    next_token = output.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "    trg_ids.append(next_token)\n",
        "\n",
        "    if next_token == trg_vocab[\"<EOS>\"]:\n",
        "      break\n",
        "\n",
        "  trg_tokens = [id_to_de[id] for id in trg_ids]\n",
        "\n",
        "  translated_sentence = \" \".join(trg_tokens[1:-1])\n",
        "\n",
        "  return translated_sentence"
      ],
      "metadata": {
        "id": "MIRDxTaGRarf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hello, how are you?\"\n",
        "\n",
        "translated_sentence = translate_sentence(\n",
        "    sentence=input_sentence,\n",
        "    model=model,\n",
        "    src_vocab=src_vocab,\n",
        "    trg_vocab=trg_vocab,\n",
        "    src_pad_idx=src_vocab[\"<PAD>\"],\n",
        "    trg_pad_idx=trg_vocab[\"<PAD>\"],\n",
        "    max_length=50,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Translated Sentence: {translated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ODRIyVFGRyvt",
        "outputId": "c3169279-1af6-47e8-dbb8-a7248cc2bbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'translate_sentence' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-602f77443b1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m translated_sentence = translate_sentence(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msrc_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'translate_sentence' is not defined"
          ]
        }
      ]
    }
  ]
}